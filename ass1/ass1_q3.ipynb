{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 With sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "import numpy as np\n",
    "\n",
    "# instances\n",
    "instances = [\n",
    "    [0.5, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [2, 0.5, 1]\n",
    "]\n",
    "\n",
    "# define symbols (latex notation)\n",
    "theta_0, theta_1, theta_2 = symbols('\\\\theta_0 \\\\theta_1 \\\\theta_2')\n",
    "x_1, x_2 = symbols('x_1 x_2')\n",
    "\n",
    "# define functions\n",
    "H = theta_0 +  theta_1 * x_1 + theta_2 * x_2  # target function\n",
    "H_0 = symbols('H_0')  # prediction\n",
    "SE = (H - H_0) ** 2  # squared error\n",
    "MSE = sum(map(lambda v: SE.subs({'x_1': v[0], 'x_2': v[1], 'H_0': v[2]})/3, instances))  # mean squared error\n",
    "\n",
    "# calculate partial derivatives\n",
    "diffs = list(map(lambda theta: diff(MSE, theta), [theta_0, theta_1, theta_2]))\n",
    "\n",
    "# initial assignment\n",
    "thetas = np.array([0.5, 0.5, 0.5])\n",
    "eta = 1\n",
    "\n",
    "# gradient descent\n",
    "def gd():\n",
    "    global thetas\n",
    "\n",
    "    sub = {\n",
    "        '\\\\theta_0': thetas[0],\n",
    "        '\\\\theta_1': thetas[1],\n",
    "        '\\\\theta_2': thetas[2],\n",
    "    }  # substitution map\n",
    "\n",
    "    print(\"thetas   : \" + str(thetas))\n",
    "    print(\"loss     : \" + str(MSE.subs(sub)))\n",
    "\n",
    "    grads = np.array(list(map(lambda diff: diff.subs(sub), diffs)))\n",
    "    thetas = thetas - eta * grads\n",
    "    print(\"gradients: \" + str(grads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \n",
       "    H = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2}         \t(1)\\\\\n",
       "    J = \\frac{\\left(\\theta_{0} + \\theta_{1}\\right)^{2}}{3} + \\frac{4 \\left(\\frac{\\theta_{0}}{2} + \\theta_{1} + 0.25 \\theta_{2} - \\frac{1}{2}\\right)^{2}}{3} + \\frac{\\left(\\theta_{0} + 0.5 \\theta_{1} + \\theta_{2} - 1\\right)^{2}}{3}       (2)\\\\\n",
       "    \\frac{\\partial J}{\\partial \\theta_0} = 2 \\theta_{0} + 2.33333333333333 \\theta_{1} + 1.0 \\theta_{2} - \\frac{4}{3}    (3)\\\\\n",
       "    \\frac{\\partial J}{\\partial \\theta_1} = 2.33333333333333 \\theta_{0} + 3.5 \\theta_{1} + 1.0 \\theta_{2} - 1.66666666666667    (4)\\\\\n",
       "    \\frac{\\partial J}{\\partial \\theta_2} = 1.0 \\theta_{0} + 1.0 \\theta_{1} + 0.833333333333333 \\theta_{2} - 1.0    (5)\\\\\n",
       "    $"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy.interactive import printing\n",
    "from IPython.display import Math\n",
    "\n",
    "Math(\n",
    "    f\"\"\"\n",
    "    H = {printing.default_latex(H)}         (1)\\\\\\\\\n",
    "    J = {printing.default_latex(MSE)}       (2)\\\\\\\\\n",
    "    \\\\frac{{\\partial J}}{{\\\\partial \\\\theta_0}} = {printing.default_latex(diffs[0])}    (3)\\\\\\\\\n",
    "    \\\\frac{{\\partial J}}{{\\\\partial \\\\theta_1}} = {printing.default_latex(diffs[1])}    (4)\\\\\\\\\n",
    "    \\\\frac{{\\partial J}}{{\\\\partial \\\\theta_2}} = {printing.default_latex(diffs[2])}    (5)\\\\\\\\\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thetas   : [0.5 0.5 0.5]\n",
      "loss     : 0.541666666666667\n",
      "gradients: [1.33333333333333 1.75000000000000 0.416666666666667]\n"
     ]
    }
   ],
   "source": [
    "# can run this multiple time\n",
    "# n=1 is too high and will diverge\n",
    "\n",
    "gd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 With Pytorch Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([0.5, 0.5, 0.5], requires_grad=True)\n",
    "X = torch.tensor(\n",
    "    [[1, 0.5, 1],\n",
    "     [1, 1, 0],\n",
    "     [1, 2, 0.5]]\n",
    ")\n",
    "Y = torch.tensor([1, 0, 1])\n",
    "\n",
    "def gd_autograd(eta = 0.1):\n",
    "    global W, X, Y\n",
    "\n",
    "    print(f\"W: {W}\")\n",
    "\n",
    "    # calculate loss\n",
    "    Y_hat = torch.matmul(X, W)\n",
    "    loss = torch.square(Y - Y_hat).sum() / 3\n",
    "    print(f\"Loss: {loss}\")\n",
    "    \n",
    "    # gradient descent\n",
    "    loss.backward()\n",
    "    update = - eta * W.grad\n",
    "    print(f\"Update: {update}\")\n",
    "\n",
    "    # update\n",
    "    W = W + update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: tensor([0.5000, 0.5000, 0.5000], requires_grad=True)\n",
      "Loss: 0.5416666865348816\n",
      "Update: tensor([-1.3333, -1.7500, -0.4167])\n"
     ]
    }
   ],
   "source": [
    "gd_autograd(eta=1)  # TODO: this call only be called once"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de36a10c7c8d30eb45029e4a1c915783a59b9e994e138dff877910840b716401"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
